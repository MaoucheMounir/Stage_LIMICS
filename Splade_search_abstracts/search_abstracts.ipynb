{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from splade.models.transformer_rep import Splade\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splade_bow(doc, top=None):\n",
    "    # now compute the document representation\n",
    "    with torch.no_grad():\n",
    "        doc_rep = model(d_kwargs=tokenizer(doc, return_tensors=\"pt\"))[\"d_rep\"].squeeze()  # (sparse) doc rep in voc space, shape (30522,)\n",
    "\n",
    "    # get the number of non-zero dimensions in the rep:\n",
    "    col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "    #print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "    # now let's inspect the bow representation:\n",
    "    weights = doc_rep[col].cpu().tolist()\n",
    "    d = {k: v for k, v in zip(col, weights)}\n",
    "    sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    bow_rep = []\n",
    "    \n",
    "    sorted_d_items = list(sorted_d.items())[:top] if top is not None else sorted_d.items()\n",
    "    \n",
    "    for k, v in sorted_d_items:\n",
    "        bow_rep.append((reverse_voc[k], round(v, 2)))\n",
    "    return bow_rep\n",
    "\n",
    "def get_next_phrase(text:str, idx_start=None):\n",
    "    idx_point = text.find(\".\", idx_start)\n",
    "    \n",
    "    if idx_start is None:\n",
    "        idx_start = 0\n",
    "    return text[idx_start:idx_point+1], idx_point+1 \n",
    "\n",
    "def diviser_passages(text_fr) -> list[str]:\n",
    "    passages = []\n",
    "    indice = 0\n",
    "\n",
    "    while indice < len(text_fr):\n",
    "        \n",
    "        passage = str()\n",
    "        while(len(passage) < 512) and indice < len(text_fr):\n",
    "        \n",
    "            indice_precedent = indice\n",
    "\n",
    "            phrase, indice = get_next_phrase(text_fr, indice)\n",
    "            if len(passage) + len(phrase) < 512: \n",
    "                passage += \" \" + phrase\n",
    "            else:\n",
    "                indice = indice_precedent\n",
    "                break\n",
    "        passages.append(passage)\n",
    "\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getcwd().endswith(\"\\\\_data_mounir\\\\PDFs\"):\n",
    "    os.chdir(\"_data_mounir\\\\PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(src_path, dest_path):\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.mkdir(dest_path)\n",
    "    \n",
    "    for filename in os.listdir(src_path):\n",
    "        with open (os.path.join(src_path, filename), \"r\") as f:\n",
    "            text_eng = f.read()\n",
    "        \n",
    "        text_eng = text_eng.replace(\"\\n\", \" \")\n",
    "        doc_split2 = diviser_passages(text_eng)\n",
    "        \n",
    "        bows = []\n",
    "        for passage in doc_split2:\n",
    "            bows.append(get_splade_bow(passage, top=50))\n",
    "            \n",
    "        df = pd.DataFrame(zip(doc_split2, bows), columns=[\"text\", \"bow_rep\"])\n",
    "        #df.text = df.text.apply(lambda x: x.replace(\"\\n\",\" \"))\n",
    "        \n",
    "        ##\n",
    "        bow_rep_all = set()\n",
    "        for bow in df.bow_rep:\n",
    "            bow_rep_all = bow_rep_all.union(bow)\n",
    "            \n",
    "        # Regrouper les termes identiques et considérer uniquement l'instance du terme avec le plus grand score\n",
    "        df_all = pd.DataFrame(bow_rep_all, columns=[\"term\", \"weight\"])\n",
    "        idx = df_all.groupby('term')['weight'].idxmax()\n",
    "        # Sélectionner les lignes correspondantes dans le DataFrame original\n",
    "        df_clean = df_all.loc[idx].reset_index(drop=True)\n",
    "        df_clean.to_csv(os.path.join(dest_path, filename[:-4]+\"-all.csv\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(\"Abstracts\", \"Vectors_all_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser une requete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, path_vectors=\"Vectors_all\"):\n",
    "    query_vectorized = get_splade_bow(query, top=50)\n",
    "    df_query = pd.DataFrame(query_vectorized, columns=[\"term\", \"weight\"])\n",
    "    scores_files = []\n",
    "    \n",
    "    for file in os.listdir(path_vectors): \n",
    "        score_doc = 0\n",
    "        df = pd.read_csv(os.path.join(path_vectors, file))\n",
    "        # Transformer le DataFrame en dictionnaire avec les colonnes\n",
    "        dict = df.set_index('term')['weight'].to_dict()\n",
    "        \n",
    "        for term, weight in df_query.itertuples(index=False):\n",
    "            score_mot_doc = dict.get(term, 0)\n",
    "            score_doc += score_mot_doc*weight\n",
    "        scores_files.append((file, score_doc))\n",
    "    sorted_scores = sorted(scores_files, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_scores, df_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  23.615099999999998),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  8.3425),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  0.5903999999999999),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  0.582),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  0.5409),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  0.4419),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv',\n",
       "  0.39050000000000007),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  0.14),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  0.1215),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  0.0846),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  0.021400000000000002),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 0.0),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  0.0),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  0.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"singular value decomposition\"\n",
    "\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  41.6353),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  15.2043),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  13.940400000000002),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  11.0157),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  7.917299999999999),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  7.642500000000001),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  5.959899999999999),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  5.4123),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  4.922999999999999),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  4.7474),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 4.723000000000001),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  3.2698),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv',\n",
       "  1.8153000000000001),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  1.5568)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning\"\n",
    "# Mots clés provenant du papier de CODER\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  17.9271),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 11.770499999999997),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  11.262099999999998),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  10.0911),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  9.7262),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  9.458600000000002),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  6.6984),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  6.2185999999999995),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  6.1054),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  5.557000000000001),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv', 5.0354),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  5.015500000000001),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  3.0707),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv', 0.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"document retrieval, medical informatics, clinical phenotypes\"\n",
    "# Mots clés provenant du papier de from doc2vec ...\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  22.344899999999996),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  11.856800000000002),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 4.7299999999999995),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  2.4709000000000003),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  2.4185),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  2.2472),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  1.9546),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  1.5811),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  1.2644),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  1.2031),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv', 0.763),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  0.6930000000000001),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  0.6765),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  0.255)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"DEFT, LLM, RAG, prompt, LoRA, Apollo.\"\n",
    "# Mots clés provenant du papier de LIMICS@DEFT2021\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  23.8007),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  17.0502),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  14.702199999999996),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  14.3023),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  11.6194),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  11.0844),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  10.5338),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 10.289600000000002),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  9.659799999999999),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  6.3093),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  5.098599999999999),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv', 4.5433),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  3.7782),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  3.5342000000000002)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Large language model, retrieval-augmented generation, natural language processing, information retrieval\"\n",
    "# Mots clés provenant du papier de RAG for LLMS a Survey\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  16.1892),\n",
       " ('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  15.787800000000002),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  15.038),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  9.373000000000001),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  9.311600000000002),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv',\n",
       "  7.7642999999999995),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 6.3988),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  6.120400000000001),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  5.5911),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  4.810299999999999),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  4.7938),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  2.0998),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  1.8203),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  0.1911)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Fast Retrieval, Sparse Representation, BERT\"\n",
    "# Mots clés provenant du papier de SparTERM\n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SPLADE Sparse Lexical and Expansion Model for First Stage Ranking-vectorized2-all.csv',\n",
       "  18.025399999999998),\n",
       " ('SPLADE v2 Sparse Lexical and Expansion Model for Information Retrieval-vectorized2-all.csv',\n",
       "  14.2064),\n",
       " ('SparTerm - Learning Term-based Sparse Representation for Fast Text Retrieval-vectorized2-all.csv',\n",
       "  8.883399999999998),\n",
       " ('Colbert V2 - Effective and Efficient Retrieval via Lightweight Late Interaction-vectorized2-all.csv',\n",
       "  7.3327),\n",
       " ('Document vectorization Method using network information of words-vectorized2-all.csv',\n",
       "  3.7853),\n",
       " ('Distributed representations of words and phrases-vectorized2-all.csv',\n",
       "  3.6971),\n",
       " ('CODER - Knowledge infused cross-lingual medical term embedding for term normalization-vectorized2-all.csv',\n",
       "  3.4678),\n",
       " ('From doc2vec to advanced keyword queries_ searching for phenotypes in large clinical document databases-vectorized2-all.csv',\n",
       "  2.9975000000000005),\n",
       " ('On the use of Singular Value Decomposition for Text Retrieval-vectorized2-all.csv',\n",
       "  2.3463000000000003),\n",
       " ('DocBERT BERT for Document Classification-vectorized2-all.csv',\n",
       "  1.1143000000000003),\n",
       " ('Retrieval Augmented Generation for LLMs - A survey-vectorized2-all.csv',\n",
       "  0.6463000000000001),\n",
       " ('Automatic Cohort Retrieval-vectorized2-all.csv', 0.5486),\n",
       " ('LIMICS@DEFT’24 - Un mini-LLM peut-il tricher aux QCM de pharmacie en fouillant dans Wikipédia et NACHOS-vectorized2-all.csv',\n",
       "  0.2804),\n",
       " ('The Unifed Medical Language System - Integrating biomedical terminology-vectorized2-all.csv',\n",
       "  0.124)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"neural networks, indexing, sparse representations, regularization\"\n",
    "# Mots clés provenant du papier de SPLADE et SPLADE 2   \n",
    "scores, df_query = retrieve(query)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splade_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
