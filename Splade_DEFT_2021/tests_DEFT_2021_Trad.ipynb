{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-priority",
   "metadata": {},
   "source": [
    "# SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tight-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from splade.models.transformer_rep import Splade\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mighty-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dir for trained weights\n",
    "\n",
    "##### v2\n",
    "# model_type_or_dir = \"naver/splade_v2_max\"\n",
    "# model_type_or_dir = \"naver/splade_v2_distil\"\n",
    "\n",
    "### v2bis, directly download from Hugging Face\n",
    "# model_type_or_dir = \"naver/splade-cocondenser-selfdistil\"\n",
    "model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c071a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splade_bow(doc, top=None):\n",
    "    # now compute the document representation\n",
    "    with torch.no_grad():\n",
    "        doc_rep = model(d_kwargs=tokenizer(doc, return_tensors=\"pt\"))[\"d_rep\"].squeeze()  # (sparse) doc rep in voc space, shape (30522,)\n",
    "\n",
    "    # get the number of non-zero dimensions in the rep:\n",
    "    col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "    #print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "    # now let's inspect the bow representation:\n",
    "    weights = doc_rep[col].cpu().tolist()\n",
    "    d = {k: v for k, v in zip(col, weights)}\n",
    "    sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    bow_rep = []\n",
    "    \n",
    "    sorted_d_items = list(sorted_d.items())[:top] if top is not None else sorted_d.items()\n",
    "    \n",
    "    for k, v in sorted_d_items:\n",
    "        bow_rep.append((reverse_voc[k], round(v, 2)))\n",
    "    return bow_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121bf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getcwd().endswith(\"\\\\_data_mounir\\\\DEFT2021\"):\n",
    "    os.chdir(\"_data_mounir\\\\DEFT2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_phrase(text:str, idx_start=None):\n",
    "    idx_point = text.find(\".\", idx_start)\n",
    "    \n",
    "    if idx_start is None:\n",
    "        idx_start = 0\n",
    "    return text[idx_start:idx_point+1], idx_point+1 \n",
    "\n",
    "def diviser_passages(text_fr) -> list[str]:\n",
    "    passages = []\n",
    "    indice = 0\n",
    "\n",
    "    while indice < len(text_fr):\n",
    "        \n",
    "        passage = str()\n",
    "        while(len(passage) < 512) and indice < len(text_fr):\n",
    "        \n",
    "            indice_precedent = indice\n",
    "        \n",
    "            phrase, indice = get_next_phrase(text_fr, indice)\n",
    "            if len(passage) + len(phrase) < 512: \n",
    "                passage += \" \" + phrase\n",
    "            else:\n",
    "                indice = indice_precedent\n",
    "                break\n",
    "        passages.append(passage)\n",
    "\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44fe3a",
   "metadata": {},
   "source": [
    "# Obsolètes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c5b5a",
   "metadata": {},
   "source": [
    "Avec blocs de 2 phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e1ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"txt_trad\"):\n",
    "    with open (os.path.join(\"txt_trad\", filename), \"r\") as f:\n",
    "        text_eng = f.read()\n",
    "    \n",
    "    doc_split = text_eng.split(sep='.')\n",
    "    doc_split2 = [doc_split[i]+doc_split[i+1] for i in range(0, len(doc_split)-1, 2)]\n",
    "    \n",
    "    bows = []\n",
    "    for passage in doc_split2:\n",
    "        bows.append(get_splade_bow(passage, top=20))\n",
    "        \n",
    "    df = pd.DataFrame(zip(doc_split2, bows), columns=[\"text\", \"bow_rep\"])\n",
    "    df.text = df.text.apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    df.to_csv(filename+\"-vectorized.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de122fb0",
   "metadata": {},
   "source": [
    "Avec blocs de 512 caracteres max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62e8bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"txt_trad\"):\n",
    "    with open (os.path.join(\"txt_trad\", filename), \"r\") as f:\n",
    "        text_eng = f.read()\n",
    "    \n",
    "    \n",
    "    doc_split2 = diviser_passages(text_eng)\n",
    "    \n",
    "    bows = []\n",
    "    for passage in doc_split2:\n",
    "        bows.append(get_splade_bow(passage, top=20))\n",
    "        \n",
    "    df = pd.DataFrame(zip(doc_split2, bows), columns=[\"text\", \"bow_rep\"])\n",
    "    df.text = df.text.apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    df.to_csv(filename+\"-vectorized2.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0b4e2",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215cfae",
   "metadata": {},
   "source": [
    "Extraire les 50 termes les plus représentatifs pour chaque passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84293c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"txt_trad\"):\n",
    "    with open (os.path.join(\"txt_trad\", filename), \"r\") as f:\n",
    "        text_eng = f.read()\n",
    "    \n",
    "    \n",
    "    doc_split2 = diviser_passages(text_eng)\n",
    "    \n",
    "    bows = []\n",
    "    for passage in doc_split2:\n",
    "        bows.append(get_splade_bow(passage, top=50))\n",
    "        \n",
    "    df = pd.DataFrame(zip(doc_split2, bows), columns=[\"text\", \"bow_rep\"])\n",
    "    df.text = df.text.apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    #df.to_csv(filename+\"-vectorized3.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d8bbf",
   "metadata": {},
   "source": [
    "Regrouper les termes des passages d'un même document dans un seul vecteur (on fait l'union de tout) (Quand les termes se répètent dans des passages différents, on prend celui avec le meilleur score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "583908b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"txt_trad_vectorized3all\"):\n",
    "    os.mkdir(\"txt_trad_vectorized3all\")\n",
    "\n",
    "\n",
    "for file in os.listdir(\"txt_trad_vectorized3\"):\n",
    "    bow_rep_all = set()\n",
    "    df = pd.read_csv(os.path.join(\"txt_trad_vectorized3\", file))\n",
    "    df.bow_rep = df.bow_rep.apply(ast.literal_eval)\n",
    "    \n",
    "    for bow in df.bow_rep:\n",
    "        bow_rep_all = bow_rep_all.union(bow)\n",
    "    \n",
    "    # Regrouper les termes identiques et considérer uniquement l'instance du terme avec le plus grand score\n",
    "    df = pd.DataFrame(bow_rep_all, columns=[\"term\", \"weight\"])\n",
    "    idx = df.groupby('term')['weight'].idxmax()\n",
    "    # Sélectionner les lignes correspondantes dans le DataFrame original\n",
    "    df_clean = df.loc[idx].reset_index(drop=True)\n",
    "    df.to_csv(os.path.join(\"txt_trad_vectorized3all\", file+\"-vectorized-all.csv\"))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "606da4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('##ac', 0.92),\n",
       " ('##ain', 1.46),\n",
       " ('##ani', 1.22),\n",
       " ('##bag', 1.29),\n",
       " ('##bar', 1.26),\n",
       " ('##bet', 1.74),\n",
       " ('##bin', 0.91),\n",
       " ('##bit', 1.87),\n",
       " ('##ble', 0.92),\n",
       " ('##cation', 1.05),\n",
       " ('##cial', 1.15),\n",
       " ('##cr', 1.18),\n",
       " ('##cture', 1.43),\n",
       " ('##dic', 1.92),\n",
       " ('##dine', 2.27),\n",
       " ('##ea', 0.89),\n",
       " ('##ech', 1.41),\n",
       " ('##ection', 1.66),\n",
       " ('##elial', 1.3),\n",
       " ('##elo', 1.57),\n",
       " ('##ema', 1.44),\n",
       " ('##ema', 1.68),\n",
       " ('##ema', 1.79),\n",
       " ('##emia', 1.39),\n",
       " ('##eno', 1.72),\n",
       " ('##ergy', 0.89),\n",
       " ('##esis', 1.53),\n",
       " ('##eter', 1.56),\n",
       " ('##eth', 1.63),\n",
       " ('##eu', 1.72),\n",
       " ('##fa', 1.01),\n",
       " ('##far', 1.68),\n",
       " ('##fusion', 1.57),\n",
       " ('##gia', 1.41),\n",
       " ('##ginal', 1.4),\n",
       " ('##gonal', 1.57),\n",
       " ('##gram', 1.37),\n",
       " ('##gui', 2.22),\n",
       " ('##he', 1.24),\n",
       " ('##hg', 1.54),\n",
       " ('##hoe', 1.2),\n",
       " ('##hra', 1.83),\n",
       " ('##hyl', 1.08),\n",
       " ('##ia', 1.33),\n",
       " ('##ical', 1.47),\n",
       " ('##id', 1.21),\n",
       " ('##ina', 1.27),\n",
       " ('##ina', 1.31),\n",
       " ('##ination', 1.54),\n",
       " ('##ino', 1.52),\n",
       " ('##it', 1.41),\n",
       " ('##it', 1.71),\n",
       " ('##ital', 1.59),\n",
       " ('##ium', 1.64),\n",
       " ('##ivated', 1.26),\n",
       " ('##less', 1.86),\n",
       " ('##llin', 1.12),\n",
       " ('##llo', 0.95),\n",
       " ('##logies', 1.62),\n",
       " ('##los', 1.59),\n",
       " ('##lva', 1.78),\n",
       " ('##ma', 1.29),\n",
       " ('##mba', 0.99),\n",
       " ('##mba', 1.43),\n",
       " ('##mour', 1.29),\n",
       " ('##mp', 1.01),\n",
       " ('##mur', 1.85),\n",
       " ('##nia', 1.5),\n",
       " ('##no', 1.28),\n",
       " ('##oca', 1.25),\n",
       " ('##oca', 1.26),\n",
       " ('##ocytes', 1.18),\n",
       " ('##od', 0.87),\n",
       " ('##off', 1.12),\n",
       " ('##og', 0.86),\n",
       " ('##ogen', 1.27),\n",
       " ('##ological', 1.27),\n",
       " ('##ology', 1.49),\n",
       " ('##ona', 1.45),\n",
       " ('##oni', 2.01),\n",
       " ('##op', 1.19),\n",
       " ('##opa', 1.56),\n",
       " ('##ope', 1.24),\n",
       " ('##ota', 1.03),\n",
       " ('##oth', 1.3),\n",
       " ('##ove', 1.38),\n",
       " ('##pa', 1.18),\n",
       " ('##pa', 1.2),\n",
       " ('##par', 1.44),\n",
       " ('##pati', 1.86),\n",
       " ('##pha', 0.82),\n",
       " ('##phy', 1.46),\n",
       " ('##pit', 2.01),\n",
       " ('##po', 1.77),\n",
       " ('##pod', 1.82),\n",
       " ('##pods', 1.06),\n",
       " ('##py', 1.14),\n",
       " ('##rad', 1.6),\n",
       " ('##rc', 1.27),\n",
       " ('##rdial', 0.99),\n",
       " ('##re', 0.94),\n",
       " ('##ril', 1.74),\n",
       " ('##ril', 1.88),\n",
       " ('##ro', 1.11),\n",
       " ('##roid', 1.26),\n",
       " ('##rom', 1.48),\n",
       " ('##rosis', 1.3),\n",
       " ('##rr', 1.31),\n",
       " ('##rr', 1.51),\n",
       " ('##rus', 1.43),\n",
       " ('##sal', 1.03),\n",
       " ('##san', 1.52),\n",
       " ('##sco', 1.61),\n",
       " ('##sco', 1.62),\n",
       " ('##scopic', 1.61),\n",
       " ('##scopic', 1.76),\n",
       " ('##sic', 1.37),\n",
       " ('##sic', 1.46),\n",
       " ('##static', 1.96),\n",
       " ('##sto', 1.37),\n",
       " ('##sto', 1.73),\n",
       " ('##tal', 1.6),\n",
       " ('##te', 1.1),\n",
       " ('##thy', 1.17),\n",
       " ('##tic', 1.11),\n",
       " ('##time', 1.05),\n",
       " ('##tri', 1.18),\n",
       " ('##tro', 1.36),\n",
       " ('##tur', 1.9),\n",
       " ('##tur', 2.03),\n",
       " ('##tur', 2.17),\n",
       " ('##u', 1.0),\n",
       " ('##uk', 1.22),\n",
       " ('##ula', 1.28),\n",
       " ('##uma', 1.29),\n",
       " ('##ure', 0.78),\n",
       " ('##ured', 1.92),\n",
       " ('##uro', 0.88),\n",
       " ('##va', 1.19),\n",
       " ('##valent', 1.62),\n",
       " ('##ves', 1.44),\n",
       " ('##vi', 1.37),\n",
       " ('##xa', 1.33),\n",
       " ('##xa', 1.35),\n",
       " ('##xi', 1.44),\n",
       " ('##ym', 1.38),\n",
       " ('##yn', 1.28),\n",
       " ('##yna', 1.36),\n",
       " ('##yre', 2.49),\n",
       " ('##ys', 1.35),\n",
       " ('##ys', 1.45),\n",
       " ('##yte', 0.95),\n",
       " ('##ytic', 1.34),\n",
       " ('##ze', 1.3),\n",
       " ('%', 0.93),\n",
       " ('19', 0.93),\n",
       " ('1991', 1.76),\n",
       " ('1a', 1.16),\n",
       " ('3', 1.24),\n",
       " ('33', 1.19),\n",
       " ('3a', 1.73),\n",
       " ('400', 0.94),\n",
       " ('45', 1.17),\n",
       " ('48', 1.37),\n",
       " ('50', 1.11),\n",
       " ('50', 1.19),\n",
       " ('500', 0.93),\n",
       " ('58', 1.38),\n",
       " ('60', 1.46),\n",
       " ('70', 1.12),\n",
       " ('abdomen', 1.02),\n",
       " ('abdominal', 1.34),\n",
       " ('abnormal', 1.34),\n",
       " ('access', 1.83),\n",
       " ('accident', 1.24),\n",
       " ('accidents', 1.03),\n",
       " ('acute', 1.85),\n",
       " ('aden', 1.25),\n",
       " ('aden', 1.55),\n",
       " ('admission', 1.16),\n",
       " ('admission', 1.61),\n",
       " ('admitted', 1.26),\n",
       " ('admitted', 1.37),\n",
       " ('age', 0.96),\n",
       " ('age', 0.99),\n",
       " ('age', 1.04),\n",
       " ('age', 1.09),\n",
       " ('age', 1.13),\n",
       " ('age', 1.15),\n",
       " ('age', 1.3),\n",
       " ('age', 1.37),\n",
       " ('age', 1.49),\n",
       " ('aircraft', 1.02),\n",
       " ('airline', 1.54),\n",
       " ('allergic', 1.56),\n",
       " ('amy', 1.31),\n",
       " ('analysis', 0.97),\n",
       " ('ang', 1.55),\n",
       " ('anterior', 1.84),\n",
       " ('ap', 0.99),\n",
       " ('arthritis', 1.68),\n",
       " ('asbestos', 2.08),\n",
       " ('associated', 1.18),\n",
       " ('associated', 1.2),\n",
       " ('aug', 1.14),\n",
       " ('august', 1.3),\n",
       " ('aviation', 1.42),\n",
       " ('bag', 1.04),\n",
       " ('balance', 1.93),\n",
       " ('bed', 0.94),\n",
       " ('bilateral', 1.99),\n",
       " ('biological', 1.29),\n",
       " ('biological', 1.54),\n",
       " ('birth', 0.82),\n",
       " ('blood', 1.12),\n",
       " ('born', 1.15),\n",
       " ('bp', 1.4),\n",
       " ('brain', 0.85),\n",
       " ('breathing', 0.96),\n",
       " ('bud', 1.78),\n",
       " ('calcium', 1.72),\n",
       " ('cardiac', 0.89),\n",
       " ('cardiovascular', 1.15),\n",
       " ('cave', 2.2),\n",
       " ('caves', 1.38),\n",
       " ('ce', 1.19),\n",
       " ('cerebral', 0.99),\n",
       " ('ch', 1.4),\n",
       " ('change', 0.98),\n",
       " ('chest', 1.54),\n",
       " ('chest', 1.62),\n",
       " ('child', 1.2),\n",
       " ('children', 0.91),\n",
       " ('chronic', 1.31),\n",
       " ('clinical', 1.06),\n",
       " ('company', 0.95),\n",
       " ('complex', 0.77),\n",
       " ('complicated', 1.29),\n",
       " ('con', 1.25),\n",
       " ('confirmed', 0.96),\n",
       " ('confirmed', 1.07),\n",
       " ('consist', 1.01),\n",
       " ('consult', 1.03),\n",
       " ('consultation', 1.7),\n",
       " ('cr', 0.79),\n",
       " ('cr', 1.81),\n",
       " ('ct', 1.4),\n",
       " ('cub', 1.84),\n",
       " ('cy', 1.56),\n",
       " ('d', 0.89),\n",
       " ('d', 1.09),\n",
       " ('d', 1.28),\n",
       " ('daily', 0.97),\n",
       " ('daily', 1.05),\n",
       " ('degree', 1.21),\n",
       " ('department', 1.45),\n",
       " ('detect', 0.87),\n",
       " ('dia', 1.12),\n",
       " ('diabetes', 1.15),\n",
       " ('diabetes', 1.46),\n",
       " ('distress', 1.45),\n",
       " ('dl', 1.0),\n",
       " ('do', 1.08),\n",
       " ('dome', 2.27),\n",
       " ('dos', 1.17),\n",
       " ('dose', 1.06),\n",
       " ('drug', 0.91),\n",
       " ('e', 1.8),\n",
       " ('early', 0.91),\n",
       " ('electro', 1.04),\n",
       " ('electro', 1.55),\n",
       " ('eliminate', 1.43),\n",
       " ('end', 1.15),\n",
       " ('eng', 1.3),\n",
       " ('episode', 1.15),\n",
       " ('est', 1.47),\n",
       " ('estimated', 1.22),\n",
       " ('eu', 1.41),\n",
       " ('evidence', 1.06),\n",
       " ('examination', 0.92),\n",
       " ('examination', 1.0),\n",
       " ('examination', 1.04),\n",
       " ('examination', 1.13),\n",
       " ('except', 0.99),\n",
       " ('exposed', 0.97),\n",
       " ('exposure', 1.19),\n",
       " ('extension', 1.64),\n",
       " ('external', 1.62),\n",
       " ('failure', 1.22),\n",
       " ('fe', 1.05),\n",
       " ('feb', 1.6),\n",
       " ('feb', 1.66),\n",
       " ('female', 0.92),\n",
       " ('fever', 1.45),\n",
       " ('first', 1.03),\n",
       " ('first', 1.35),\n",
       " ('fist', 1.79),\n",
       " ('flank', 2.35),\n",
       " ('flex', 1.49),\n",
       " ('flexible', 1.85),\n",
       " ('fracture', 1.4),\n",
       " ('fractures', 1.32),\n",
       " ('free', 1.98),\n",
       " ('front', 1.16),\n",
       " ('frontal', 2.04),\n",
       " ('functions', 0.94),\n",
       " ('g', 1.31),\n",
       " ('gen', 1.03),\n",
       " ('go', 1.56),\n",
       " ('gram', 1.15),\n",
       " ('grams', 1.76),\n",
       " ('ha', 0.99),\n",
       " ('ha', 1.28),\n",
       " ('ha', 1.44),\n",
       " ('had', 0.91),\n",
       " ('had', 0.96),\n",
       " ('had', 1.01),\n",
       " ('hard', 1.07),\n",
       " ('he', 0.97),\n",
       " ('he', 1.33),\n",
       " ('headache', 1.81),\n",
       " ('heart', 0.9),\n",
       " ('heart', 1.25),\n",
       " ('hem', 1.05),\n",
       " ('hem', 1.11),\n",
       " ('het', 1.31),\n",
       " ('high', 0.97),\n",
       " ('his', 0.75),\n",
       " ('history', 0.97),\n",
       " ('history', 1.07),\n",
       " ('history', 1.3),\n",
       " ('homogeneous', 2.11),\n",
       " ('hospital', 0.97),\n",
       " ('hospital', 1.06),\n",
       " ('hospital', 1.12),\n",
       " ('hospitalized', 1.04),\n",
       " ('hospitalized', 1.14),\n",
       " ('hospitals', 0.88),\n",
       " ('hume', 1.8),\n",
       " ('hydro', 1.43),\n",
       " ('hyper', 1.01),\n",
       " ('hyper', 1.48),\n",
       " ('infectious', 1.18),\n",
       " ('intense', 1.44),\n",
       " ('intermittent', 2.0),\n",
       " ('intra', 1.64),\n",
       " ('irregular', 1.38),\n",
       " ('isolated', 1.86),\n",
       " ('isolation', 1.38),\n",
       " ('k', 1.89),\n",
       " ('kidney', 1.59),\n",
       " ('l', 1.63),\n",
       " ('le', 1.04),\n",
       " ('left', 1.39),\n",
       " ('left', 1.43),\n",
       " ('les', 1.21),\n",
       " ('les', 1.52),\n",
       " ('lesions', 1.15),\n",
       " ('lesions', 1.37),\n",
       " ('level', 1.06),\n",
       " ('limb', 1.26),\n",
       " ('limbs', 0.97),\n",
       " ('limits', 0.87),\n",
       " ('lip', 1.4),\n",
       " ('liver', 1.31),\n",
       " ('lobe', 1.94),\n",
       " ('long', 0.78),\n",
       " ('long', 1.05),\n",
       " ('lower', 0.97),\n",
       " ('lower', 1.37),\n",
       " ('lung', 0.89),\n",
       " ('lung', 1.25),\n",
       " ('lung', 1.52),\n",
       " ('macro', 1.94),\n",
       " ('macro', 1.99),\n",
       " ('male', 1.34),\n",
       " ('man', 1.13),\n",
       " ('management', 1.03),\n",
       " ('marriage', 1.51),\n",
       " ('married', 1.06),\n",
       " ('mass', 1.36),\n",
       " ('mass', 1.73),\n",
       " ('masses', 1.06),\n",
       " ('masses', 1.24),\n",
       " ('maxi', 1.68),\n",
       " ('mc', 1.52),\n",
       " ('medication', 1.27),\n",
       " ('mg', 1.28),\n",
       " ('microscopic', 1.49),\n",
       " ('mig', 1.48),\n",
       " ('mm', 1.31),\n",
       " ('mobile', 1.2),\n",
       " ('mr', 1.21),\n",
       " ('mr', 1.87),\n",
       " ('ms', 1.85),\n",
       " ('multiple', 1.63),\n",
       " ('my', 0.89),\n",
       " ('nasal', 1.59),\n",
       " ('nausea', 1.7),\n",
       " ('neurological', 1.35),\n",
       " ('no', 0.96),\n",
       " ('no', 1.34),\n",
       " ('no', 1.92),\n",
       " ('norm', 1.35),\n",
       " ('normal', 1.23),\n",
       " ('normal', 1.7),\n",
       " ('nose', 1.04),\n",
       " ('obstruction', 1.57),\n",
       " ('occupational', 1.77),\n",
       " ('old', 0.78),\n",
       " ('old', 1.19),\n",
       " ('old', 1.22),\n",
       " ('old', 1.34),\n",
       " ('old', 1.45),\n",
       " ('old', 1.47),\n",
       " ('old', 1.51),\n",
       " ('old', 1.55),\n",
       " ('old', 1.6),\n",
       " ('old', 1.63),\n",
       " ('old', 1.64),\n",
       " ('os', 1.49),\n",
       " ('pain', 1.03),\n",
       " ('pain', 1.06),\n",
       " ('pain', 1.12),\n",
       " ('pain', 1.33),\n",
       " ('painful', 1.04),\n",
       " ('paint', 1.42),\n",
       " ('painter', 1.2),\n",
       " ('painting', 1.47),\n",
       " ('pal', 1.33),\n",
       " ('pal', 1.5),\n",
       " ('pal', 1.57),\n",
       " ('par', 1.36),\n",
       " ('path', 1.64),\n",
       " ('patient', 1.25),\n",
       " ('patients', 0.84),\n",
       " ('peripheral', 1.19),\n",
       " ('persistent', 1.45),\n",
       " ('persistent', 1.53),\n",
       " ('ph', 0.79),\n",
       " ('physical', 1.31),\n",
       " ('pi', 1.5),\n",
       " ('pit', 1.48),\n",
       " ('pits', 1.72),\n",
       " ('placed', 1.04),\n",
       " ('polar', 1.84),\n",
       " ('poly', 1.17),\n",
       " ('poly', 1.95),\n",
       " ('pressure', 1.04),\n",
       " ('pressure', 1.67),\n",
       " ('pressure', 1.91),\n",
       " ('pro', 1.14),\n",
       " ('prostate', 1.98),\n",
       " ('public', 1.23),\n",
       " ('pulmonary', 1.88),\n",
       " ('pun', 1.21),\n",
       " ('r', 1.01),\n",
       " ('radius', 1.74),\n",
       " ('rec', 1.65),\n",
       " ('renal', 1.38),\n",
       " ('renal', 1.51),\n",
       " ('renal', 1.52),\n",
       " ('renal', 1.57),\n",
       " ('res', 1.42),\n",
       " ('respiratory', 1.29),\n",
       " ('responsible', 1.15),\n",
       " ('rest', 1.34),\n",
       " ('results', 0.99),\n",
       " ('retain', 1.21),\n",
       " ('retain', 1.32),\n",
       " ('retention', 1.75),\n",
       " ('retention', 1.84),\n",
       " ('retro', 2.02),\n",
       " ('rhino', 1.73),\n",
       " ('right', 1.33),\n",
       " ('right', 1.65),\n",
       " ('right', 1.69),\n",
       " ('right', 1.82),\n",
       " ('rigid', 1.06),\n",
       " ('risk', 1.07),\n",
       " ('road', 1.35),\n",
       " ('scan', 1.25),\n",
       " ('seizure', 1.28),\n",
       " ('seizures', 1.42),\n",
       " ('sensitive', 1.27),\n",
       " ('sensitivity', 1.77),\n",
       " ('sent', 1.15),\n",
       " ('sent', 1.33),\n",
       " ('she', 1.05),\n",
       " ('side', 1.05),\n",
       " ('side', 1.29),\n",
       " ('sixty', 1.12),\n",
       " ('specific', 1.04),\n",
       " ('stable', 1.12),\n",
       " ('stage', 1.46),\n",
       " ('start', 0.98),\n",
       " ('status', 1.7),\n",
       " ('stomach', 1.49),\n",
       " ('subjected', 1.13),\n",
       " ('suffered', 0.93),\n",
       " ('surgery', 1.37),\n",
       " ('surgical', 1.09),\n",
       " ('symptoms', 1.08),\n",
       " ('sync', 1.9),\n",
       " ('syndrome', 0.94),\n",
       " ('thyroid', 1.05),\n",
       " ('touch', 1.68),\n",
       " ('touch', 1.74),\n",
       " ('touched', 1.0),\n",
       " ('toxic', 1.58),\n",
       " ('tr', 1.42),\n",
       " ('trauma', 1.22),\n",
       " ('traumatic', 0.96),\n",
       " ('tumor', 1.51),\n",
       " ('ul', 1.14),\n",
       " ('ultra', 0.98),\n",
       " ('ultra', 1.04),\n",
       " ('ultrasound', 1.44),\n",
       " ('ultrasound', 1.51),\n",
       " ('under', 0.92),\n",
       " ('upper', 1.11),\n",
       " ('upper', 1.26),\n",
       " ('ur', 0.88),\n",
       " ('ur', 1.27),\n",
       " ('ur', 1.38),\n",
       " ('ur', 1.41),\n",
       " ('ur', 1.66),\n",
       " ('urine', 1.22),\n",
       " ('urine', 1.47),\n",
       " ('urine', 1.48),\n",
       " ('usual', 1.37),\n",
       " ('va', 1.14),\n",
       " ('vascular', 1.0),\n",
       " ('vitamin', 1.61),\n",
       " ('vomit', 1.35),\n",
       " ('vomiting', 1.58),\n",
       " ('vu', 1.44),\n",
       " ('warning', 0.98),\n",
       " ('warning', 1.06),\n",
       " ('without', 1.02),\n",
       " ('without', 1.23),\n",
       " ('woman', 0.92),\n",
       " ('woman', 0.95),\n",
       " ('woman', 1.1),\n",
       " ('women', 0.96),\n",
       " ('women', 1.0),\n",
       " ('yes', 0.99)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_rep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18b10f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##ac</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>##ain</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##ani</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##bag</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##bar</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term  weight\n",
       "0   ##ac    0.92\n",
       "1  ##ain    1.46\n",
       "2  ##ani    1.22\n",
       "3  ##bag    1.29\n",
       "4  ##bar    1.26"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(bow_rep_all, columns=[\"term\", \"weight\"])\n",
    "idx = df.groupby('term')['weight'].idxmax()\n",
    "# Sélectionner les lignes correspondantes dans le DataFrame original\n",
    "df_clean = df.loc[idx].reset_index(drop=True)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5974794",
   "metadata": {},
   "source": [
    "## Vectoriser une requête et calculer les similarités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b516bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\splade_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##gm</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##fusion</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distress</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>normal</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respiratory</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>##mun</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>##bu</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>%</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>im</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>breathing</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dose</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>severe</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grams</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>percent</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>g</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cds</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>##lin</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>die</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>death</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mg</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lung</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>##og</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ml</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>injection</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>given</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>total</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>##lins</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>/</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>equation</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>died</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>administered</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>patients</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>level</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>calculated</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>weight</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>calculate</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>distressed</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>##lo</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>test</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ratio</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>liter</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>##ulin</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  weight\n",
       "0            ##gm    2.26\n",
       "1              cd    2.25\n",
       "2        ##fusion    1.82\n",
       "3        distress    1.80\n",
       "4               i    1.68\n",
       "5              19    1.58\n",
       "6          normal    1.58\n",
       "7     respiratory    1.48\n",
       "8           ##mun    1.46\n",
       "9            ##bu    1.28\n",
       "10              %    1.28\n",
       "11             im    1.22\n",
       "12            max    1.21\n",
       "13              l    1.20\n",
       "14      breathing    1.17\n",
       "15           dose    1.14\n",
       "16         severe    1.14\n",
       "17          grams    1.13\n",
       "18        percent    1.10\n",
       "19              g    1.09\n",
       "20            cds    1.09\n",
       "21          ##lin    1.07\n",
       "22            die    1.07\n",
       "23          death    1.07\n",
       "24        patient    1.02\n",
       "25             17    1.00\n",
       "26             mg    0.99\n",
       "27           lung    0.98\n",
       "28           ##og    0.98\n",
       "29             ml    0.97\n",
       "30      injection    0.94\n",
       "31          given    0.94\n",
       "32          total    0.94\n",
       "33         ##lins    0.89\n",
       "34              /    0.86\n",
       "35       equation    0.84\n",
       "36           died    0.83\n",
       "37   administered    0.81\n",
       "38       patients    0.78\n",
       "39          level    0.78\n",
       "40     calculated    0.74\n",
       "41         weight    0.70\n",
       "42      calculate    0.70\n",
       "43     distressed    0.69\n",
       "44           ##lo    0.64\n",
       "45           test    0.63\n",
       "46          ratio    0.61\n",
       "47          liter    0.61\n",
       "48         ##ulin    0.60\n",
       "49  concentration    0.58"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"urine kidney blood pressure\"\n",
    "\n",
    "#filepdf-190-cas.txt-vectorized.csv\n",
    "\n",
    "query_vectorized = get_splade_bow(passage, top=50)\n",
    "df_query = pd.DataFrame(query_vectorized, columns=[\"term\", \"weight\"])\n",
    "df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dd9db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"txt_trad_vectorized3all\"\n",
    "for file in os.listdir(path):\n",
    "    df = pd.read_csv(os.path.join(path, file))\n",
    "    df_termes_communs = df[df['term'].isin(df_query.term.tolist())]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9830c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##gm\n",
      "cd\n",
      "##fusion\n",
      "distress\n",
      "i\n",
      "19\n",
      "normal\n",
      "respiratory\n",
      "##mun\n",
      "##bu\n",
      "%\n",
      "im\n",
      "max\n",
      "l\n",
      "breathing\n",
      "dose\n",
      "severe\n",
      "grams\n",
      "percent\n",
      "g\n",
      "cds\n",
      "##lin\n",
      "die\n",
      "death\n",
      "patient\n",
      "17\n",
      "mg\n",
      "lung\n",
      "##og\n",
      "ml\n",
      "injection\n",
      "given\n",
      "total\n",
      "##lins\n",
      "/\n",
      "equation\n",
      "died\n",
      "administered\n",
      "patients\n",
      "level\n",
      "calculated\n",
      "weight\n",
      "calculate\n",
      "distressed\n",
      "##lo\n",
      "test\n",
      "ratio\n",
      "liter\n",
      "##ulin\n",
      "concentration\n"
     ]
    }
   ],
   "source": [
    "for term, weight in df_query.itertuples(index=False):\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f330f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"txt_trad_vectorized3all\"\n",
    "scores_files = []\n",
    "for file in os.listdir(path):\n",
    "    score_doc = 0\n",
    "    df = pd.read_csv(os.path.join(path, file))\n",
    "    # Transformer le DataFrame en dictionnaire avec les colonnes\n",
    "    dict = df.set_index('term')['weight'].to_dict()\n",
    "    \n",
    "    for term, weight in df_query.itertuples(index=False):\n",
    "      score_mot_doc = dict.get(term, 0)\n",
    "      score_doc += score_mot_doc*weight\n",
    "    scores_files.append((file, score_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dbac7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filepdf-119-cas.txt-vectorized3.csv-vectorized-all.csv', 9.4788),\n",
       " ('filepdf-144-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  9.076799999999999),\n",
       " ('filepdf-176-cas.txt-vectorized3.csv-vectorized-all.csv', 16.3914),\n",
       " ('filepdf-190-cas.txt-vectorized3.csv-vectorized-all.csv', 4.002400000000001),\n",
       " ('filepdf-263-3-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  3.7336000000000005),\n",
       " ('filepdf-32-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  8.533999999999999),\n",
       " ('filepdf-472-cas.txt-vectorized3.csv-vectorized-all.csv', 18.536),\n",
       " ('filepdf-700-cas.txt-vectorized3.csv-vectorized-all.csv', 4.1617),\n",
       " ('filepdf-702-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  4.8629999999999995),\n",
       " ('filepdf-705-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  22.185299999999994),\n",
       " ('filepdf-798-5-cas.txt-vectorized-all.csv', 63.2388),\n",
       " ('filepdf-798-5-cas.txt-vectorized3.csv-vectorized-all.csv', 63.2388)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "12c516ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filepdf-798-5-cas.txt-vectorized-all.csv', 63.2388),\n",
       " ('filepdf-798-5-cas.txt-vectorized3.csv-vectorized-all.csv', 63.2388),\n",
       " ('filepdf-705-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  22.185299999999994),\n",
       " ('filepdf-472-cas.txt-vectorized3.csv-vectorized-all.csv', 18.536),\n",
       " ('filepdf-176-cas.txt-vectorized3.csv-vectorized-all.csv', 16.3914),\n",
       " ('filepdf-119-cas.txt-vectorized3.csv-vectorized-all.csv', 9.4788),\n",
       " ('filepdf-144-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  9.076799999999999),\n",
       " ('filepdf-32-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  8.533999999999999),\n",
       " ('filepdf-702-2-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  4.8629999999999995),\n",
       " ('filepdf-700-cas.txt-vectorized3.csv-vectorized-all.csv', 4.1617),\n",
       " ('filepdf-190-cas.txt-vectorized3.csv-vectorized-all.csv', 4.002400000000001),\n",
       " ('filepdf-263-3-cas.txt-vectorized3.csv-vectorized-all.csv',\n",
       "  3.7336000000000005)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores = sorted(scores_files, key=lambda x: x[1], reverse=True)\n",
    "sorted_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
